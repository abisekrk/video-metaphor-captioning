{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q decord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-11 12:11:01,083] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import torch\n",
    "device = \"cuda:1\"\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-large-vatex\", device=device, type=torch.bfloat16)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-large-vatex\", output_hidden_states=True, torch_dtype=torch.bfloat16).to(device)\n",
    "# processor = AutoProcessor.from_pretrained(\"microsoft/git-large-msrvtt-qa\", device=device, type=torch.bfloat16)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-large-msrvtt-qa\", output_hidden_states=True, torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"microsoft/git-large-vatex\", device=device)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-large-vatex\", output_hidden_states=True).to(device)\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"microsoft/git-base-msrvtt-qa\", device=device)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-msrvtt-qa\", output_hidden_states=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.git.modeling_git.GitForCausalLM"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "num_frames = model.config.num_image_with_embedding\n",
    "print(num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llava/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/envs/llava/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from decord import VideoReader, cpu\n",
    "from torchvision import transforms\n",
    "from einops import rearrange\n",
    "\n",
    "def sample_frames(file_path, num_frames):\n",
    "    # set seed for reproducability\n",
    "    np.random.seed(45)\n",
    "\n",
    "    def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "        converted_len = int(clip_len * frame_sample_rate)\n",
    "        end_idx = np.random.randint(converted_len, seg_len)\n",
    "        start_idx = end_idx - converted_len\n",
    "        indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "        return indices\n",
    "\n",
    "    # video clip consists of 300 frames (10 seconds at 30 FPS)\n",
    "    videoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n",
    "\n",
    "    # sample 6 frames\n",
    "    videoreader.seek(0)\n",
    "    indices = sample_frame_indices(clip_len=num_frames, frame_sample_rate=4, seg_len=len(videoreader))\n",
    "    frames = videoreader.get_batch(indices).asnumpy()\n",
    "\n",
    "    return list(frames)\n",
    "\n",
    "def make_video_from_image(file_path, num_frames):\n",
    "    image = Image.open(file_path)\n",
    "    image.load()\n",
    "    data = np.asarray(image)\n",
    "    frames = [data] * num_frames\n",
    "    # data = transforms.ToTensor()(image)\n",
    "    # data = rearrange(data, 'c b h -> b h c')\n",
    "    # # .permute(2, 0, 1)\n",
    "    # frames = [data] * num_frames\n",
    "    return frames\n",
    "\n",
    "\n",
    "# video_path = \"/research/video_metaphor/data/videos/6AR0xq6g5Uo.mp4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_frames(file_path, num_frames):\n",
    "    # set seed for reproducability\n",
    "    np.random.seed(45)\n",
    "\n",
    "    def get_frames_in_interval(start, end, number_of_frames):\n",
    "        indices = np.random.randint(low = start, high = end, size = number_of_frames)\n",
    "        indices = np.sort(indices)\n",
    "        return indices\n",
    "\n",
    "\n",
    "    def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "        # Making three intervals: Start, middle and the end\n",
    "        interval_gap = seg_len // 3\n",
    "        frames_per_gap = clip_len // 3\n",
    "        \n",
    "        begin_interval = get_frames_in_interval(0, interval_gap, frames_per_gap)\n",
    "        middle_interval = get_frames_in_interval(interval_gap + 1, interval_gap * 2, frames_per_gap)\n",
    "        end_interval = get_frames_in_interval((interval_gap*2) + 1, seg_len, frames_per_gap)\n",
    "        frame_indices = np.concatenate((begin_interval, middle_interval, end_interval), axis = 0)\n",
    "        print(frame_indices)\n",
    "        return frame_indices\n",
    "    \n",
    "\n",
    "\n",
    "    # video clip consists of 300 frames (10 seconds at 30 FPS)\n",
    "    videoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n",
    "\n",
    "    # sample 6 frames\n",
    "    videoreader.seek(0)\n",
    "    indices = sample_frame_indices(clip_len=num_frames, frame_sample_rate=4, seg_len=len(videoreader))\n",
    "    frames = videoreader.get_batch(indices).asnumpy()\n",
    "    print(frames)\n",
    "\n",
    "    return list(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling multiple frames in each interval\n",
    "def sample_frames_on_interval(file_path, num_frames):\n",
    "    \n",
    "    def get_frames_in_interval(start, end, number_of_frames):\n",
    "        indices = np.random.randint(low = start, high = end, size = number_of_frames)\n",
    "        indices = np.sort(indices)\n",
    "        return indices\n",
    "\n",
    "\n",
    "    # def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    #     # Making three intervals: Start, middle and the end\n",
    "    #     interval_gap = seg_len // 3\n",
    "    #     frames_per_gap = 6\n",
    "        \n",
    "    #     begin_interval = get_frames_in_interval(0, interval_gap, frames_per_gap)\n",
    "    #     middle_interval = get_frames_in_interval(interval_gap + 1, interval_gap * 2, frames_per_gap)\n",
    "    #     end_interval = get_frames_in_interval((interval_gap*2) + 1, seg_len, frames_per_gap)\n",
    "    #     # frame_indices = np.concatenate((begin_interval, middle_interval, end_interval), axis = 0)\n",
    "    #     # print(begin_interval)\n",
    "    #     frames = [begin_interval, middle_interval, end_interval]\n",
    "    #     print(frames)\n",
    "    #     return frames\n",
    "\n",
    "    def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "        # Making three intervals: Start, middle and the end\n",
    "        interval_gap = seg_len // 16\n",
    "        frames_per_gap = 6\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        for i in range(0, 16):\n",
    "            curr_interval = get_frames_in_interval(interval_gap * i, interval_gap * (i+1), frames_per_gap)\n",
    "            print(\"Current interval: \",interval_gap * i, interval_gap * (i+1) )\n",
    "            print(\"chosen frames: \")\n",
    "            print(curr_interval)\n",
    "            frames.append(curr_interval)\n",
    "        \n",
    "        \n",
    "        print(frames)\n",
    "        return frames\n",
    "    \n",
    "    videoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n",
    "\n",
    "    # sample 6 frames\n",
    "    videoreader.seek(0)\n",
    "    indices = sample_frame_indices(clip_len=num_frames, frame_sample_rate=4, seg_len=len(videoreader))\n",
    "    frame_set = []\n",
    "    for index_set in indices:\n",
    "        video_frames = videoreader.get_batch(index_set).asnumpy()\n",
    "        frame_set.append(list(video_frames))\n",
    "\n",
    "    return frame_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current interval:  0 56\n",
      "chosen frames: \n",
      "[ 7 16 19 21 36 45]\n",
      "Current interval:  56 112\n",
      "chosen frames: \n",
      "[ 69  75  94  95 106 108]\n",
      "Current interval:  112 168\n",
      "chosen frames: \n",
      "[112 122 124 135 137 166]\n",
      "Current interval:  168 224\n",
      "chosen frames: \n",
      "[178 180 197 213 219 221]\n",
      "Current interval:  224 280\n",
      "chosen frames: \n",
      "[239 244 244 246 268 273]\n",
      "Current interval:  280 336\n",
      "chosen frames: \n",
      "[292 305 309 327 329 335]\n",
      "Current interval:  336 392\n",
      "chosen frames: \n",
      "[338 347 352 362 365 368]\n",
      "Current interval:  392 448\n",
      "chosen frames: \n",
      "[392 396 399 400 408 442]\n",
      "Current interval:  448 504\n",
      "chosen frames: \n",
      "[453 461 463 465 470 476]\n",
      "Current interval:  504 560\n",
      "chosen frames: \n",
      "[517 518 528 539 542 543]\n",
      "Current interval:  560 616\n",
      "chosen frames: \n",
      "[560 564 571 574 577 591]\n",
      "Current interval:  616 672\n",
      "chosen frames: \n",
      "[640 657 660 662 663 671]\n",
      "Current interval:  672 728\n",
      "chosen frames: \n",
      "[675 676 687 710 723 726]\n",
      "Current interval:  728 784\n",
      "chosen frames: \n",
      "[731 751 754 759 772 779]\n",
      "Current interval:  784 840\n",
      "chosen frames: \n",
      "[789 795 811 819 825 838]\n",
      "Current interval:  840 896\n",
      "chosen frames: \n",
      "[842 848 861 865 877 886]\n",
      "[array([ 7, 16, 19, 21, 36, 45]), array([ 69,  75,  94,  95, 106, 108]), array([112, 122, 124, 135, 137, 166]), array([178, 180, 197, 213, 219, 221]), array([239, 244, 244, 246, 268, 273]), array([292, 305, 309, 327, 329, 335]), array([338, 347, 352, 362, 365, 368]), array([392, 396, 399, 400, 408, 442]), array([453, 461, 463, 465, 470, 476]), array([517, 518, 528, 539, 542, 543]), array([560, 564, 571, 574, 577, 591]), array([640, 657, 660, 662, 663, 671]), array([675, 676, 687, 710, 723, 726]), array([731, 751, 754, 759, 772, 779]), array([789, 795, 811, 819, 825, 838]), array([842, 848, 861, 865, 877, 886])]\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "video_path = \"/research/video_metaphor/data/videos/L4mP9pR-mzU.mp4\"\n",
    "\n",
    "frames = sample_frames_on_interval(video_path, num_frames)\n",
    "print(len(frames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 640, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 640, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path = \"/research/video_metaphor/data/videos/L4mP9pR-mzU.mp4\"\n",
    "\n",
    "frames = sample_frames(video_path, num_frames)\n",
    "frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(\"/research/video_metaphor/LLaVA/playground/data/irfl/metaphor_images/00afd9d5cce899ad9979c8ed9f42fc99_2.jpg\")\n",
    "img.load()\n",
    "data = np.asarray(img)\n",
    "print(data.shape)\n",
    "frames = [data] * num_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "frames = make_video_from_image(\"/research/video_metaphor/LLaVA/playground/data/irfl/metaphor_images/00afd9d5cce899ad9979c8ed9f42fc99_2.jpg\", num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 640, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 640, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\"\n",
    "# outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs[\u001b[39m'\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "inputs['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = \"/research/video_metaphor/data/videos/L4mP9pR-mzU.mp4\"\n",
    "\n",
    "# frames = sample_frames(video_path, num_frames)\n",
    "inputs = processor(images=frames, return_tensors=\"pt\").to(device)\n",
    "\n",
    "question = \"What colour shirt is th lady wearing?\"\n",
    "\n",
    "input_ids = processor(text=question, add_special_tokens=False).input_ids\n",
    "input_ids = [processor.tokenizer.cls_token_id] + input_ids\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "input_ids = input_ids.to(device)#.to(torch.bfloat16)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, pixel_values=inputs.pixel_values.to(device).to(torch.bfloat16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:149: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  return torch.tensor(value)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 0 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;66;03m#.to(torch.bfloat16)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/git/modeling_git.py:1499\u001b[0m, in \u001b[0;36mGitForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, pixel_values, head_mask, inputs_embeds, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1497\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1499\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1513\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1514\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(sequence_output)\n",
      "File \u001b[0;32m/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/git/modeling_git.py:1275\u001b[0m, in \u001b[0;36mGitModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, pixel_values, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1270\u001b[0m projected_visual_features \u001b[38;5;241m=\u001b[39m projected_visual_features\u001b[38;5;241m.\u001b[39mrepeat(\n\u001b[1;32m   1271\u001b[0m     embedding_output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m projected_visual_features\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1272\u001b[0m )\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;66;03m# concatenate patch token and text token embeddings\u001b[39;00m\n\u001b[0;32m-> 1275\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprojected_visual_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;66;03m# By default, an additive causal mask is created\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;66;03m# for masking the future (one direction).\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_future_mask(seq_length, embedding_output\u001b[38;5;241m.\u001b[39mdtype, embedding_output\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 0 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# video_path = \"/research/video_metaphor/data/videos/L4mP9pR-mzU.mp4\"\n",
    "\n",
    "# frames = sample_frames(video_path, num_frames)\n",
    "inputs = processor(images=frames, return_tensors=\"pt\").to(device)\n",
    "\n",
    "input_ids = [processor.tokenizer.cls_token_id]\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "input_ids = input_ids.to(device)#.to(torch.bfloat16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, pixel_values=inputs.pixel_values.to(device).to(torch.bfloat16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values'])\n"
     ]
    }
   ],
   "source": [
    "# Testing for training\n",
    "inputs = processor(images=frames, text=[\"The guy is playing football\"],return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(inputs.keys())\n",
    "# input_ids = [processor.tokenizer.cls_token_id]\n",
    "# input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "# input_ids = input_ids.to(device)#.to(torch.bfloat16)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(input_ids, pixel_values=inputs.pixel_values.to(device).to(torch.bfloat16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: ['a woman is showing a snickers brand and then a snickers is shown.']\n"
     ]
    }
   ],
   "source": [
    "# video_path = \"/research/video_metaphor/data/videos/L4mP9pR-mzU.mp4\"\n",
    "\n",
    "# frames = sample_frames(video_path, num_frames)\n",
    "inputs = processor(images=frames[5], return_tensors=\"pt\").to(device)\n",
    "\n",
    "input_ids = [processor.tokenizer.cls_token_id]\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "input_ids = input_ids.to(device)#.to(torch.bfloat16)\n",
    "generated_ids = model.generate(pixel_values=inputs.pixel_values.to(device).to(torch.bfloat16),input_ids=input_ids, max_length=50, output_hidden_states=True)\n",
    "print(\"Generated caption:\", processor.batch_decode(generated_ids, skip_special_tokens=True))\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(input_ids, pixel_values=inputs.pixel_values.to(device).to(torch.bfloat16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hidden states for multiple parts of videos:\n",
    "all_outputs = []\n",
    "\n",
    "\n",
    "input_ids = [processor.tokenizer.cls_token_id]\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "input_ids = input_ids.to(device)#.to(torch.bfloat16)\n",
    "\n",
    "for frame in frames:\n",
    "    inputs = processor(images=frame, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, pixel_values=inputs.pixel_values.to(device).to(torch.bfloat16))\n",
    "        hidden_states = outputs.hidden_states[-1].to(torch.bfloat16)\n",
    "        all_outputs.append(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "<class 'list'>\n",
      "torch.Size([1, 1543, 768])\n",
      "torch.Size([1, 1543, 768])\n",
      "tensor([[  8.3750, -30.8750, -26.7500,  ...,  17.0000,  -7.0000,  -5.3125],\n",
      "        [  8.8750, -13.1250, -10.8750,  ...,  14.1250,  14.3750,  -4.4688],\n",
      "        [  8.8125, -10.0000, -10.9375,  ...,  12.8125,  17.5000,  -8.0000],\n",
      "        ...,\n",
      "        [  5.0000, -14.1250,   0.5234,  ...,   8.5625,  29.0000, -13.8750],\n",
      "        [ 11.0000,  -4.5938,  -9.0000,  ...,  10.6250,  21.3750,  -4.4375],\n",
      "        [ -1.9531, -21.2500, -35.0000,  ...,   3.7656,  10.7500, -26.6250]],\n",
      "       device='cuda:1', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(len(all_outputs))\n",
    "print(type(all_outputs))\n",
    "print(all_outputs[0].shape)\n",
    "# all_hidden_states = torch.cat(all_outputs, dim=0)\n",
    "all_hidden_states = sum(all_outputs)\n",
    "# all_hidden_states = torch.div(all_hidden_states, 3)\n",
    "print(all_hidden_states.shape)\n",
    "print(all_hidden_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  8.3750, -30.8750, -26.6250,  ...,  17.1250,  -6.9688,  -5.3125],\n",
       "         [  8.8750, -13.0625, -10.8125,  ...,  14.0625,  14.4375,  -4.4375],\n",
       "         [  8.7500,  -9.9375, -10.8750,  ...,  12.8750,  17.5000,  -8.0625],\n",
       "         ...,\n",
       "         [  5.0000, -14.1250,   0.5078,  ...,   8.5625,  28.8750, -13.8125],\n",
       "         [ 10.8750,  -4.5625,  -9.0625,  ...,  10.6250,  21.3750,  -4.4375],\n",
       "         [ -1.9531, -21.2500, -34.7500,  ...,   3.7500,  10.7500, -26.7500]]],\n",
       "       device='cuda:1', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.stack(all_outputs)\n",
    "y = x.sum(dim=0)\n",
    "y.shape\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1543, 768])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden size: 1190, 768]\n",
    "t=outputs.hidden_states[-1].to(torch.bfloat16)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [t, t]\n",
    "results_tensor = torch.cat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_outputs = outputs.logits\n",
    "forward_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(pixel_values=inputs.pixel_values.to(device).to(torch.bfloat16),input_ids=input_ids, max_length=50, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: ['a man is showing a snickers brand and then a woman is shown talking.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated caption:\", processor.batch_decode(generated_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking if all video files can be loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"/research/video_metaphor/LLaVA/playground/data/video_data/video300_train.json\",'r')\n",
    "dataset = json.load(file)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    file_path = data[\"video\"]\n",
    "    try:\n",
    "        videoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n",
    "    except:\n",
    "        print(\"Failed for : \", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/raid/nlp/abisekrk/abisek/research/video_metaphor/data/videos/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
