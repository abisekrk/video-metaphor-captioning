{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"llava\", \"llava_x\", \"git\", \"valley\", \"videochatgpt\", \"video_llava\"]\n",
    "mapping_texts = [\"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 50\n"
     ]
    }
   ],
   "source": [
    "with open(f\"/research/video_metaphor/LLaVA/playground/data/video_data/manual_eval_june/outputs/ann1_op.json\", 'r') as file:\n",
    "    evaluations1 = json.load(file)\n",
    "\n",
    "with open(f\"/research/video_metaphor/LLaVA/playground/data/video_data/manual_eval_june/outputs/ann2_op.json\", 'r') as file:\n",
    "    evaluations2 = json.load(file)\n",
    "\n",
    "with open(f\"/research/video_metaphor/LLaVA/playground/data/video_data/manual_eval_june/outputs/ann3_op.json\", 'r') as file:\n",
    "    evaluations3 = json.load(file)\n",
    "\n",
    "\n",
    "print(len(evaluations1), len(evaluations2), len(evaluations3))\n",
    "# evaluations1[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_evaluations = evaluations1 + evaluations2 + evaluations3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_basepath = \"/research/video_metaphor/LLaVA/playground/data/video_data/manual_eval_june/permutation_mapping/\"\n",
    "\n",
    "scores = {\n",
    "    \"fluency\" : {},\n",
    "    \"creativity\": {},\n",
    "    \"pcc\": {},\n",
    "    \"consistency\": {}\n",
    "}\n",
    "\n",
    "for key, value in scores.items():\n",
    "    for mod in models:\n",
    "        scores[key][mod] = 0\n",
    "# print(scores)\n",
    "\n",
    "for count in range(1,151):\n",
    "    # print(count)\n",
    "    if count in range(26, 51) or count in range(76, 101):\n",
    "        # For the videos annotated by all 3 annotators (last 25 of each 50 videos)\n",
    "        # Ignoring annotations of 2 annotators, so that they don't overweigh the final results\n",
    "        continue\n",
    "    permutation_count = count % 50\n",
    "    if permutation_count == 0:\n",
    "        permutation_count = 50\n",
    "    with open(f\"{mapper_basepath}{permutation_count}.json\", 'r') as file:\n",
    "        column_mapper = json.load(file)\n",
    "    model_mapper = dict((v,k) for k,v in column_mapper.items())\n",
    "    # print(\"mapper: \")\n",
    "    # print(model_mapper)\n",
    "    evaluation_data = all_evaluations[count-1]\n",
    "\n",
    "    for key_indexer in range(1, 25):\n",
    "        if key_indexer >=1 and key_indexer <=6:\n",
    "            metric = \"fluency\"\n",
    "        elif key_indexer >=7 and key_indexer <=12:\n",
    "            metric = \"creativity\"\n",
    "        elif key_indexer >=13 and key_indexer <=18:\n",
    "            metric = \"pcc\"\n",
    "        elif key_indexer >=19 and key_indexer <=24:\n",
    "            metric = \"consistency\"\n",
    "\n",
    "        data_key_value = f\"q{key_indexer}_answer\"\n",
    "        model_ind = (key_indexer%6) - 1\n",
    "        model_id = mapping_texts[model_ind]\n",
    "        model_name = model_mapper[model_id]\n",
    "        try:\n",
    "            current_answer = evaluation_data[data_key_value]\n",
    "        except:\n",
    "            current_answer = \"No\"\n",
    "            print(count)\n",
    "            print(\"Key: \", key_indexer)\n",
    "        if current_answer == \"Yes\":\n",
    "            scores[metric][model_name]+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fluency': {'llava': 99,\n",
       "  'llava_x': 98,\n",
       "  'git': 95,\n",
       "  'valley': 36,\n",
       "  'videochatgpt': 1,\n",
       "  'video_llava': 96},\n",
       " 'creativity': {'llava': 89,\n",
       "  'llava_x': 96,\n",
       "  'git': 87,\n",
       "  'valley': 30,\n",
       "  'videochatgpt': 2,\n",
       "  'video_llava': 91},\n",
       " 'pcc': {'llava': 59,\n",
       "  'llava_x': 53,\n",
       "  'git': 12,\n",
       "  'valley': 15,\n",
       "  'videochatgpt': 10,\n",
       "  'video_llava': 74},\n",
       " 'consistency': {'llava': 22,\n",
       "  'llava_x': 15,\n",
       "  'git': 3,\n",
       "  'valley': 3,\n",
       "  'videochatgpt': 8,\n",
       "  'video_llava': 34}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric:  fluency\n",
      "Model: llava score: 0.99\n",
      "Model: llava_x score: 0.98\n",
      "Model: git score: 0.95\n",
      "Model: valley score: 0.36\n",
      "Model: videochatgpt score: 0.01\n",
      "Model: video_llava score: 0.96\n",
      "Metric:  creativity\n",
      "Model: llava score: 0.89\n",
      "Model: llava_x score: 0.96\n",
      "Model: git score: 0.87\n",
      "Model: valley score: 0.3\n",
      "Model: videochatgpt score: 0.02\n",
      "Model: video_llava score: 0.91\n",
      "Metric:  pcc\n",
      "Model: llava score: 0.59\n",
      "Model: llava_x score: 0.53\n",
      "Model: git score: 0.12\n",
      "Model: valley score: 0.15\n",
      "Model: videochatgpt score: 0.1\n",
      "Model: video_llava score: 0.74\n",
      "Metric:  consistency\n",
      "Model: llava score: 0.22\n",
      "Model: llava_x score: 0.15\n",
      "Model: git score: 0.03\n",
      "Model: valley score: 0.03\n",
      "Model: videochatgpt score: 0.08\n",
      "Model: video_llava score: 0.34\n"
     ]
    }
   ],
   "source": [
    "for metric in scores.keys():\n",
    "    print(\"Metric: \", metric)\n",
    "    model_scores = scores[metric]\n",
    "    for key, value in model_scores.items():\n",
    "        print(f\"Model: {key} score: {value/100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "1200\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "def get_annotations_flattened(evaluation_set):\n",
    "    ann_scores = []\n",
    "    for data in evaluation_set:\n",
    "        for i in range(1, 25):\n",
    "            pred = data[f\"q{i}_answer\"]\n",
    "            ann_scores.append(pred)\n",
    "    print(len(ann_scores))\n",
    "    return ann_scores\n",
    "\n",
    "ann1_scores = get_annotations_flattened(evaluations1)\n",
    "ann2_scores = get_annotations_flattened(evaluations2)\n",
    "ann3_scores = get_annotations_flattened(evaluations3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohen_kappa(ann1, ann2, name):\n",
    "    \"\"\"Computes Cohen kappa for pair-wise annotators.\n",
    "    :param ann1: annotations provided by first annotator\n",
    "    :type ann1: list\n",
    "    :param ann2: annotations provided by second annotator\n",
    "    :type ann2: list\n",
    "    :rtype: float\n",
    "    :return: Cohen kappa statistic\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for an1, an2 in zip(ann1, ann2):\n",
    "        if an1 == an2:\n",
    "            count += 1\n",
    "    A = count / len(ann1)  # observed agreement A (Po)\n",
    "\n",
    "    uniq = set(ann1 + ann2)\n",
    "    E = 0  # expected agreement E (Pe)\n",
    "    for item in uniq:\n",
    "        cnt1 = ann1.count(item)\n",
    "        cnt2 = ann2.count(item)\n",
    "        count = ((cnt1 / len(ann1)) * (cnt2 / len(ann2)))\n",
    "        E += count\n",
    "\n",
    "    val = round((A - E) / (1 - E), 4)\n",
    "    print(name, \" : \", val)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 vs 2  :  0.6392\n",
      "1 vs 3  :  0.6378\n",
      "2 vs 3  :  0.685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.685"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa(ann1_scores, ann2_scores, \"1 vs 2\")\n",
    "cohen_kappa(ann1_scores, ann3_scores, \"1 vs 3\")\n",
    "cohen_kappa(ann2_scores, ann3_scores, \"2 vs 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6528666318049821"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats import inter_rater as irr\n",
    "\n",
    "\n",
    "ann1_values = [1 if i==\"Yes\" else 0 for i in ann1_scores]\n",
    "ann2_values = [1 if i==\"Yes\" else 0 for i in ann2_scores]\n",
    "ann3_values = [1 if i==\"Yes\" else 0 for i in ann3_scores]\n",
    "true_false_array = []\n",
    "\n",
    "for (x , y, z) in zip(ann1_values, ann2_values, ann3_values):\n",
    "    no_of_trues = x + y + z #no of 1 prediction\n",
    "    no_of_false = 3 - (no_of_trues)\n",
    "    true_false_array.append([no_of_trues, no_of_false])\n",
    "\n",
    "irr.fleiss_kappa(true_false_array, method='fleiss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
